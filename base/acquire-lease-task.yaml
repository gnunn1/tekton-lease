apiVersion: tekton.dev/v1beta1
kind: ClusterTask
metadata:
  name: acquire-lease
spec:
  params:
  - name: lease-name
    type: string
  - name: timeout
    type: string
    default: 20m
  steps:
    - name: create-lease
      image: image-registry.openshift-image-registry.svc:5000/openshift/cli:latest
      script: |

        # EOF in yaml is hard, so make a file the simple way
        echo 'apiVersion: demo.redhat.io/v1' > e2e-lease.yaml
        echo 'kind: Lease' >> e2e-lease.yaml
        echo 'metadata:' >> e2e-lease.yaml
        echo '  name: $(inputs.params.lease-name)' >> e2e-lease.yaml
        echo 'spec:' >> e2e-lease.yaml
        echo '  owner: ${context.pipelineRun.name)' >> e2e-lease.yaml
        echo '  created: $(date)' >> e2e-lease.yaml

        # Try to create a lease — either it succeeds, and we are good, or it fails, and then we wait for the lease to be deleted or a timeout, and then we make the lease if there was a deletion
        # In the event of a timeout, clear out the dead lease so it doesn't mess up future builds

        kubectl create -f e2e-lease.yaml || (echo Waiting for lease && kubectl wait --for=delete lease.demo.redhat.io/$(inputs.params.lease-name) --timeout=$(inputs.params.timeout) || ( echo “Grabbing abandoned lease.” && kubectl delete lease.demo.redhat.io/$(inputs.params.lease-name) ))

        # We could be here for three reasons;
        # either we successfully created a lease,
        # we waited and another run's lease got deleted,
        # or we waited and the other lease is still there.
        # Run an apply to make sure a lease with our name and label now exists.

        kubectl apply -f e2e-lease.yaml
